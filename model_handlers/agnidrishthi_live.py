import cv2
import os
import time
from datetime import datetime
from ultralytics import YOLO
import threading
from pydub import AudioSegment
from pydub.playback import play
import io
import sys
import numpy as np
import pyttsx3 # Import the pyttsx3 library

# Note: For pydub to work, you MUST have FFmpeg installed and its bin directory added to your system's PATH.
# For Windows, download from https://ffmpeg.org/download.html, extract, and add 'ffmpeg-directory/bin' to PATH.
# For Linux/macOS, use your package manager (e.g., 'sudo apt install ffmpeg' or 'brew install ffmpeg').
# Also, ensure you have PyAudio or SimpleAudio installed: pip install pyaudio (or pip install simpleaudio)

# For pyttsx3, install it: pip install pyttsx3
# Depending on your OS, you might need additional dependencies for pyttsx3's backend (e.g., espeak for Linux, NSSpeechSynthesizer for macOS, SAPI for Windows).

# Paths (These will be relative to app.py's execution, which is the project root)
BASE_YOLO_MODEL_PATH = os.path.join('yolov8_models', 'yolov8x.pt')
FIRE_YOLO_MODEL_PATH = os.path.join('yolov8_models', 'yolofirenew.pt')

# Alert Sound Paths (relative to the static folder for Flask serving)
# IMPORTANT: Ensure these files actually exist in your 'static' directory!
# E.g., C:\Users\adity\Desktop\SurakshaNethra\static\fire_alert.mp3
FIRE_SOUND_PATH = os.path.join('static', 'fire_alert.mp3')
HUMAN_SOUND_PATH = os.path.join('static', 'human_alert.mp3')
VEHICLE_SOUND_PATH = os.path.join('static', 'vehicle_alert.mp3')

# --- CONFIGURATION ---
DEFAULT_CAMERA_SOURCE = 0 # Default to webcam for local testing.

HUMAN_CLASS_NAME = 'person'
VEHICLE_CLASS_NAMES = ['car', 'truck', 'bus', 'motorcycle', 'bicycle']
FIRE_CLASS_NAME = 'Fire' # Case sensitive based on your model training, ensure it matches

CONFIDENCE_THRESHOLD_BASE = 0.5
CONFIDENCE_THRESHOLD_FIRE = 0.3

# Alert text display properties (only for the central alert message now)
ALERT_TEXT_FONT = cv2.FONT_HERSHEY_SIMPLEX
ALERT_TEXT_SCALE = 1.5
ALERT_TEXT_THICKNESS = 3
ALERT_TEXT_COLOR = (255, 255, 255) # White
ALERT_BG_COLOR_FIRE = (0, 0, 255) # Red
ALERT_BG_COLOR_HUMAN = (255, 0, 0) # Blue
ALERT_BG_COLOR_VEHICLE = (0, 255, 255) # Yellow
ALERT_BG_COLOR_HUMAN_VEHICLE = (0, 165, 255) # Orange for combined human & vehicle

# Directory for server-side logs generated by this live stream
LOGS_DIRECTORY = 'detection_logs_live'
LOG_FILE_NAME_PREFIX = 'live_detection_log_'

ALERT_SOUND_COOLDOWN_SECONDS = 5 # Cooldown for sound alerts on the server
TTS_COOLDOWN_SECONDS = 10 # Cooldown for TTS messages

# Global variable to hold the log file handle for persistent logging
LOG_FILE_HANDLE = None

# Initialize TTS engine globally (or once per thread/process if multi-threaded)
tts_engine = None
try:
    tts_engine = pyttsx3.init()
    # You can set properties like rate and volume here if needed
    # tts_engine.setProperty('rate', 150) # Speed of speech
    # tts_engine.setProperty('volume', 0.9) # Volume (0.0 to 1.0)
    # You can also get and set voices:
    # voices = tts_engine.getProperty('voices')
    # tts_engine.setProperty('voice', voices[0].id) # Change index for different voices
except Exception as e:
    print(f"WARNING: pyttsx3 initialization failed: {e}. TTS messages will not be available.")
    tts_engine = None

# --- Helper Functions ---
def get_class_id(model, class_name, captured_logs):
    """Safely gets class ID from model names, logging if not found."""
    if not hasattr(model, 'names') or not isinstance(model.names, dict):
        log_event_wrapper(f"Warning: Model does not have a valid 'names' attribute. Cannot lookup class '{class_name}'.", captured_logs)
        return None
    
    for class_id, name in model.names.items():
        if name.strip().lower() == class_name.strip().lower():
            return class_id
    log_event_wrapper(f"Warning: Class '{class_name}' not found in model names.", captured_logs)
    return None

def log_event_wrapper(msg, captured_logs):
    """Logs an event and appends it to a list (for debugging or future display)
        and writes to the global log file if open."""
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
    log_msg = f"[{ts}] {msg}"
    captured_logs.append(log_msg)

    if LOG_FILE_HANDLE and not LOG_FILE_HANDLE.closed:
        try:
            # The fix for the charmap error is here: encoding='utf-8'
            LOG_FILE_HANDLE.write(log_msg + "\n")
            LOG_FILE_HANDLE.flush() # Ensure it's written to disk immediately
        except Exception as e:
            print(f"Error writing to log file: {e}") # This will show if the write itself fails (e.g., file closed unexpectedly)

def play_sound_async_wrapper(path, captured_logs):
    """Plays a sound file asynchronously on the server using pydub."""
    if os.path.exists(path) and os.path.getsize(path) > 0: # Also check if file is not empty
        try:
            audio = AudioSegment.from_file(path)
            threading.Thread(target=play, args=(audio,), daemon=True).start()
        except Exception as e:
            log_event_wrapper(f"Error playing sound {path} with pydub: {e}. "
                              f"Ensure FFmpeg is installed and in PATH, and PyAudio/SimpleAudio is installed, "
                              f"and the audio file is a valid MP3/WAV.", captured_logs)
    else:
        log_event_wrapper(f"Warning: Sound file not found or is empty at {path}", captured_logs)

def speak_async(message, captured_logs):
    """Speaks a message asynchronously using pyttsx3."""
    if tts_engine:
        try:
            # We use a thread to avoid blocking the main frame processing loop
            threading.Thread(target=lambda: tts_engine.say(message) and tts_engine.runAndWait(), daemon=True).start()
            log_event_wrapper(f"TTS triggered: '{message}'", captured_logs)
        except Exception as e:
            log_event_wrapper(f"Error triggering TTS for message '{message}': {e}", captured_logs)
    else:
        log_event_wrapper(f"TTS engine not initialized. Cannot speak message: '{message}'", captured_logs)


def trigger_sound_alert_wrapper(alert_type, last_alert_sound_time, captured_logs):
    """
    Triggers a sound alert on the server if cooldown allows.
    Returns the updated last_alert_sound_time.
    """
    current_time = time.time()
    if alert_type and (current_time - last_alert_sound_time) >= ALERT_SOUND_COOLDOWN_SECONDS:
        path = None
        if alert_type == "FIRE":
            path = FIRE_SOUND_PATH
        elif alert_type == "HUMAN":
            path = HUMAN_SOUND_PATH
        elif alert_type == "VEHICLE":
            path = VEHICLE_SOUND_PATH
        elif alert_type == "HUMAN_VEHICLE":
            path = HUMAN_SOUND_PATH # Can use human sound for combined
            
        if path:
            play_sound_async_wrapper(path, captured_logs)
            log_event_wrapper(f"Sound alert triggered: {alert_type}", captured_logs)
            return current_time
    return last_alert_sound_time

def trigger_tts_alert_wrapper(alert_type, last_tts_alert_time, captured_logs):
    """
    Triggers a TTS alert on the server if cooldown allows.
    Returns the updated last_tts_alert_time.
    """
    current_time = time.time()
    if alert_type and (current_time - last_tts_alert_time) >= TTS_COOLDOWN_SECONDS:
        message = ""
        if alert_type == "FIRE":
            message = "Warning! Fire detected."
        elif alert_type == "HUMAN":
            message = "Alert! Human detected."
        elif alert_type == "VEHICLE":
            message = "Alert! Vehicle detected."
        elif alert_type == "HUMAN_VEHICLE":
            message = "Warning! Human and vehicle detected."
            
        if message:
            speak_async(message, captured_logs)
            return current_time
    return last_tts_alert_time


def get_blank_frame(width=640, height=480, text="ERROR: NO FEED"):
    """Generates a blank black frame with an error message."""
    blank_frame = np.zeros((height, width, 3), dtype=np.uint8)
    text_size = cv2.getTextSize(text, ALERT_TEXT_FONT, 1, 2)[0]
    text_x = (width - text_size[0]) // 2
    text_y = (height + text_size[1]) // 2
    cv2.putText(blank_frame, text, (text_x, text_y), ALERT_TEXT_FONT, 1, ALERT_TEXT_COLOR, 2)
    return blank_frame

# --- MAIN STREAMING GENERATOR FUNCTION ---
def generate_frames_with_detection(camera_source=DEFAULT_CAMERA_SOURCE, rotation_angle=0, ir_mode=False):
    """
    Generator function to capture frames, process them with YOLO models,
    and yield JPEG-encoded frames for streaming to a web browser.
    
    Args:
        camera_source: An integer (webcam index), a video file path, or an RTSP URL.
        rotation_angle: Angle to rotate the frame (0, 90, 180, 270).
        ir_mode: Boolean, true to apply IR-like colormap.
    """
    global LOG_FILE_HANDLE
    captured_logs = [] # Local buffer for logs before file is open, or if file fails

    # Setup logging to file
    try:
        if not os.path.exists(LOGS_DIRECTORY):
            os.makedirs(LOGS_DIRECTORY)
            log_event_wrapper(f"Created log directory: {LOGS_DIRECTORY}", captured_logs)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file_path = os.path.join(LOGS_DIRECTORY, f"{LOG_FILE_NAME_PREFIX}{timestamp}.txt")
        # FIX: Specify encoding='utf-8' for the log file
        LOG_FILE_HANDLE = open(log_file_path, 'a', encoding='utf-8')
        
        LOG_FILE_HANDLE.write(f"--- Live Detection Log Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ---\n")
        LOG_FILE_HANDLE.write(f"Camera Source: {camera_source}\n")
        LOG_FILE_HANDLE.write(f"Base Model: {os.path.basename(BASE_YOLO_MODEL_PATH)}\n")
        LOG_FILE_HANDLE.write(f"Fire Model: {os.path.basename(FIRE_YOLO_MODEL_PATH)}\n")
        LOG_FILE_HANDLE.write("-" * 60 + "\n")
        log_event_wrapper(f"Logging to: {log_file_path}", captured_logs)
    except Exception as e:
        log_event_wrapper(f"Failed to open log file: {e}. File logging disabled for this session.", captured_logs)
        LOG_FILE_HANDLE = None # Ensure handle is None if opening fails

    base_model = None
    fire_model = None
    try:
        if os.path.exists(BASE_YOLO_MODEL_PATH):
            base_model = YOLO(BASE_YOLO_MODEL_PATH)
            log_event_wrapper("✅ Base model loaded successfully.", captured_logs)
        else:
            log_event_wrapper(f"❌ Base model not found at {BASE_YOLO_MODEL_PATH}. Human/Vehicle detection will be skipped.", captured_logs)

        if os.path.exists(FIRE_YOLO_MODEL_PATH):
            fire_model = YOLO(FIRE_YOLO_MODEL_PATH)
            log_event_wrapper("✅ Fire model loaded successfully.", captured_logs)
        else:
            log_event_wrapper(f"❌ Fire model not found at {FIRE_YOLO_MODEL_PATH}. Fire detection will be skipped.", captured_logs)

        if base_model is None and fire_model is None:
            log_event_wrapper("❌ No YOLO models could be loaded. Detection will not function.", captured_logs)
            if LOG_FILE_HANDLE: LOG_FILE_HANDLE.close()
            yield b'--frame\r\nContent-Type: image/jpeg\r\n\r\n' + cv2.imencode('.jpg', get_blank_frame(text=f"ERROR: NO MODELS LOADED"))[1].tobytes() + b'\r\n'
            return

    except Exception as e:
        log_event_wrapper(f"❌ Model load error: {e}. Please check model paths and integrity.", captured_logs)
        if LOG_FILE_HANDLE: LOG_FILE_HANDLE.close()
        yield b'--frame\r\nContent-Type: image/jpeg\r\n\r\n' + cv2.imencode('.jpg', get_blank_frame(text=f"ERROR: MODEL LOAD FAILED - {e}"))[1].tobytes() + b'\r\n'
        return

    HUMAN_ID = get_class_id(base_model, HUMAN_CLASS_NAME, captured_logs) if base_model else None
    VEHICLE_IDS = [get_class_id(base_model, vn, captured_logs) for vn in VEHICLE_CLASS_NAMES if base_model and get_class_id(base_model, vn, captured_logs) is not None] if base_model else []
    FIRE_ID = get_class_id(fire_model, FIRE_CLASS_NAME, captured_logs) if fire_model else None

    if FIRE_ID is None and fire_model:
        log_event_wrapper(f"CRITICAL ERROR: Fire class '{FIRE_CLASS_NAME}' not found in fire model. Fire detection will not function.", captured_logs)

    cap = cv2.VideoCapture(camera_source)
    if isinstance(camera_source, str) and (camera_source.startswith("rtsp://") or camera_source.startswith("http://")):
        log_event_wrapper(f"Attempting to open IP camera stream: {camera_source}", captured_logs)
        time.sleep(2)
        for i in range(3):
            if cap.isOpened():
                log_event_wrapper(f"Successfully opened IP camera connection on attempt {i+1}.", captured_logs)
                break
            log_event_wrapper(f"Retrying IP camera connection (attempt {i+1}/3)...", captured_logs)
            cap.release()
            time.sleep(3)
            cap = cv2.VideoCapture(camera_source)
            
        if not cap.isOpened():
             log_event_wrapper(f"Failed to open IP camera source '{camera_source}' after multiple retries.", captured_logs)

    if not cap.isOpened():
        error_message = f"CRITICAL ERROR: Failed to open camera/video source '{camera_source}'. "
        if isinstance(camera_source, int):
            error_message += "Check if webcam is connected and not in use by another application."
        elif isinstance(camera_source, str) and os.path.exists(camera_source):
            error_message += "Check if video file path is correct and accessible."
        elif isinstance(camera_source, str) and (camera_source.startswith("rtsp://") or camera_source.startswith("http://")):
            error_message += "Check IP camera URL, credentials, network connectivity, or firewall settings."
        else:
            error_message += "Invalid camera source or access denied."
            
        log_event_wrapper(error_message, captured_logs)
        if LOG_FILE_HANDLE: LOG_FILE_HANDLE.close()
        yield b'--frame\r\nContent-Type: image/jpeg\r\n\r\n' + cv2.imencode('.jpg', get_blank_frame(text=f"ERROR: CAMERA/VIDEO SOURCE FAILED"))[1].tobytes() + b'\r\n'
        return

    # On-screen alert message state
    alert_msg = ""
    alert_bg = (0, 0, 0)
    alert_start_time = 0
    ALERT_DISPLAY_DURATION = 3 # seconds for on-screen alert text

    # State variables for sound and TTS alerts (cooldown)
    last_alert_sound_time = 0
    last_tts_alert_time = 0

    log_event_wrapper(f"Starting live detection stream for source: {camera_source} with rotation={rotation_angle} and IR_Mode={ir_mode}", captured_logs)

    while True:
        ret, frame = cap.read()
        if not ret:
            log_event_wrapper("Failed to grab frame or end of video stream. Stopping stream.", captured_logs)
            if isinstance(camera_source, str) and not (camera_source.startswith("rtsp://") or camera_source.startswith("http://")):
                yield b'--frame\r\nContent-Type: image/jpeg\r\n\r\n' + cv2.imencode('.jpg', get_blank_frame(text="VIDEO ENDED / DISCONNECTED"))[1].tobytes() + b'\r\n'
            break

        if frame is None: # Handle cases where frame might be None even if ret is True (rare but can happen)
            log_event_wrapper("Frame is None. Skipping frame.", captured_logs)
            continue

        # Apply rotation based on the provided argument
        if rotation_angle == 90:
            frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)
        elif rotation_angle == 180:
            frame = cv2.rotate(frame, cv2.ROTATE_180)
        elif rotation_angle == 270:
            frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)

        # Apply IR mode based on the provided argument
        if ir_mode:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            frame = cv2.applyColorMap(gray, cv2.COLORMAP_HOT)

        fire_flag = human_flag = vehicle_flag = False

        # Run base model detection
        if base_model and (HUMAN_ID is not None or VEHICLE_IDS):
            base_classes_to_detect = []
            if HUMAN_ID is not None:
                base_classes_to_detect.append(HUMAN_ID)
            base_classes_to_detect.extend(VEHICLE_IDS)

            if base_classes_to_detect:
                try:
                    res_base = base_model(frame, verbose=False, conf=CONFIDENCE_THRESHOLD_BASE, classes=base_classes_to_detect)
                    for r in res_base:
                        if len(r.boxes) > 0: # Check if any detections
                            for b in r.boxes:
                                cid = int(b.cls[0])
                                if cid == HUMAN_ID:
                                    human_flag = True
                                elif cid in VEHICLE_IDS:
                                    vehicle_flag = True
                except Exception as e:
                    log_event_wrapper(f"Error during base model inference: {e}", captured_logs)

        # Run fire model detection
        if fire_model and FIRE_ID is not None:
            try:
                res_fire = fire_model(frame, verbose=False, conf=CONFIDENCE_THRESHOLD_FIRE, classes=[FIRE_ID])
                for r in res_fire:
                    if len(r.boxes) > 0: # Check if any detections
                        fire_flag = True
            except Exception as e:
                log_event_wrapper(f"Error during fire model inference: {e}", captured_logs)

        # Determine the alert type for the current frame
        new_alert_type_current_frame = None
        if fire_flag:
            new_alert_type_current_frame = "FIRE"
        elif human_flag and vehicle_flag:
            new_alert_type_current_frame = "HUMAN_VEHICLE"
        elif human_flag:
            new_alert_type_current_frame = "HUMAN"
        elif vehicle_flag:
            new_alert_type_current_frame = "VEHICLE"
            
        # Trigger sound alert on the server, respecting cooldown
        last_alert_sound_time = trigger_sound_alert_wrapper(
            new_alert_type_current_frame, last_alert_sound_time, captured_logs
        )

        # Trigger TTS alert on the server, respecting cooldown
        last_tts_alert_time = trigger_tts_alert_wrapper(
            new_alert_type_current_frame, last_tts_alert_time, captured_logs
        )

        current_display_alert_msg = ""
        current_display_alert_bg = (0,0,0) # Default black background

        if fire_flag:
            current_display_alert_msg = "ALERT: FIRE DETECTED!"
            current_display_alert_bg = ALERT_BG_COLOR_FIRE
        elif human_flag and vehicle_flag:
            current_display_alert_msg = "ALERT: HUMAN & VEHICLE DETECTED!"
            current_display_alert_bg = ALERT_BG_COLOR_HUMAN_VEHICLE
        elif human_flag:
            current_display_alert_msg = "ALERT: HUMAN DETECTED!"
            current_display_alert_bg = ALERT_BG_COLOR_HUMAN
        elif vehicle_flag:
            current_display_alert_msg = "ALERT: VEHICLE DETECTED!"
            current_display_alert_bg = ALERT_BG_COLOR_VEHICLE

        # If there's a new alert, update display state
        if current_display_alert_msg:
            alert_msg = current_display_alert_msg
            alert_bg = current_display_alert_bg
            alert_start_time = time.time()
        # If no new alert, and existing alert's display duration has passed, clear it
        elif (time.time() - alert_start_time > ALERT_DISPLAY_DURATION):
            alert_msg = ""
            alert_bg = (0,0,0)


        # Draw the persistent alert message on the frame if active
        if alert_msg:
            # Calculate text size and position to center it
            size = cv2.getTextSize(alert_msg, ALERT_TEXT_FONT,
                                   ALERT_TEXT_SCALE, ALERT_TEXT_THICKNESS)[0]
            tx = (frame.shape[1] - size[0]) // 2
            ty = 70 # Fixed y-position for the alert text
            # Draw background rectangle for alert text
            cv2.rectangle(frame, (tx - 10, ty - size[1] - 10),
                          (tx + size[0] + 10, ty + 10), alert_bg, -1) # -1 fills the rectangle
            # Draw alert text
            cv2.putText(frame, alert_msg, (tx, ty),
                        ALERT_TEXT_FONT, ALERT_TEXT_SCALE, ALERT_TEXT_COLOR, ALERT_TEXT_THICKNESS)

        # Encode the frame to JPEG bytes and yield it
        ret, buffer = cv2.imencode('.jpg', frame)
        if not ret:
            log_event_wrapper("Failed to encode frame to JPEG. Skipping frame.", captured_logs)
            continue
        frame_bytes = buffer.tobytes()
        
        # Yield the frame in multipart/x-mixed-replace format
        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')

    # Release resources when the streaming loop breaks
    cap.release()

    if LOG_FILE_HANDLE:
        LOG_FILE_HANDLE.write("-" * 60 + "\n")
        LOG_FILE_HANDLE.write(f"--- Live Detection Stream Ended: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ---\n")
        LOG_FILE_HANDLE.close()
    log_event_wrapper("✅ Live detection stream shut down.", captured_logs) # This will now write correctly

# This `if __name__ == "__main__":` block is for local testing of this script.
if __name__ == "__main__":
    print("--- Running agnidrishthi_live.py in standalone test mode ---")
    print("This will simulate the live stream by showing frames in a CV2 window.")
    print("Press 'q' in the CV2 window to quit.")
    
    os.makedirs('static', exist_ok=True)
    os.makedirs('detection_logs_live', exist_ok=True)
    os.makedirs('yolov8_models', exist_ok=True)

    try:
        from scipy.io.wavfile import write as write_wav
        import numpy as np
        
        sample_rate = 44100
        duration = 0.5
        silent_audio = np.zeros(int(sample_rate * duration), dtype=np.int16)

        # Create dummy WAVs if MP3s are not present and scipy is available
        # Ensure the paths match the ones used by the Flask app
        for sound_file_path in [FIRE_SOUND_PATH, HUMAN_SOUND_PATH, VEHICLE_SOUND_PATH]:
            # For testing, convert the expected MP3 path to WAV for scipy.
            # In production, use real MP3s.
            if not os.path.exists(sound_file_path):
                try:
                    # Create a dummy WAV for pydub to potentially load, even if it's silent.
                    # pydub often works better with actual WAV files than super minimal MP3 headers.
                    dummy_wav_path = sound_file_path.replace('.mp3', '.wav') # Create WAV in same location
                    write_wav(dummy_wav_path, sample_rate, silent_audio)
                    print(f"Created dummy WAV sound file: {dummy_wav_path}. Please replace with actual MP3s!")
                    # NOTE: For the app to use the WAV, you'd need to adjust the FIRE_SOUND_PATH etc.
                    # It's better to just put real MP3s in the static folder.
                except Exception as e:
                    print(f"Could not create dummy WAV sound file {dummy_wav_path}: {e}")
        print("\nNOTE: Dummy WAV sound files were created for testing if MP3s were missing.")
        print("For production, please place actual MP3 alert sounds in the 'static/' folder,")
        print("e.g., 'static/fire_alert.mp3', 'static/human_alert.mp3', etc.\n")
    except ImportError:
        print("\nWARNING: SciPy not installed. Cannot create dummy WAV files for testing.")
        print("Please ensure your actual alert MP3s are in the 'static/' folder.")
        print("You can download small dummy MP3s or create them using audio software.")
        print("Or install SciPy: pip install scipy\n")


    test_camera_source = DEFAULT_CAMERA_SOURCE 
    # test_camera_source = "path/to/your/video.mp4" 

    test_rotation_angle = 0
    test_ir_mode = False

    try:
        for frame_bytes_with_header in generate_frames_with_detection(test_camera_source, test_rotation_angle, test_ir_mode):
            start_marker = b'\xff\xd8'
            end_marker = b'\xff\xd9'
            
            start_index = frame_bytes_with_header.find(start_marker)
            end_index = frame_bytes_with_header.find(end_marker, start_index)
            
            if start_index != -1 and end_index != -1:
                jpeg_bytes = frame_bytes_with_header[start_index : end_index + len(end_marker)]
                nparr = np.frombuffer(jpeg_bytes, np.uint8)
                img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

                if img is not None:
                    cv2.imshow('Live Detection Test (Press Q to Quit)', img)
                else:
                    print("Warning: Failed to decode image frame.")
            else:
                print("Warning: JPEG markers not found in frame data.")

            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break

    except Exception as e:
        print(f"An error occurred during local live stream test: {e}")
    finally:
        cv2.destroyAllWindows()
        print("--- agnidrishthi_live.py standalone test finished ---")